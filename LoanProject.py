# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G8bCsPMM57AFnpgyOBEDeJpNLdypaoMi
"""

# Commented out IPython magic to ensure Python compatibility.
# Install liberaries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# %matplotlib inline

df=pd.read_csv('/content/Dataset.csv')
df

df.head()

df.tail()

df.shape

df.columns

df.info()

df.isnull().sum()

## checking the outline values

plt.figure(figsize=(12,8))
sns.boxplot(data=df)

## Fill the null values of numerical datatype
df['LoanAmount']=df['LoanAmount'].fillna(df['LoanAmount'].median())
df['Loan_Amount_Term']=df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mean())
df['Credit_History']=df['Credit_History'].fillna(df['Credit_History'].mean())

## Fill the. null values of object data_Type
df['Gender']=df['Gender'].fillna(df['Gender'].mode()[0])
df['Married']=df['Married'].fillna(df['Married'].mode()[0])
df['Dependents']=df['Dependents'].fillna(df['Dependents'].mode()[0])
df['Self_Employed']=df['Self_Employed'].fillna(df['Self_Employed'].mode()[0])

df.isnull().sum()

print('Number of people who took loan by Gender')
print(df['Gender'].value_counts())
sns.countplot(x='Gender',data=df,palette='Set1')

print('Number of Married people who took loan ')
print(df['Married'].value_counts())
sns.countplot(x='Married',data=df,palette='Set1')

print('Number of people who took loan by Education')
print(df['Education'].value_counts())
sns.countplot(x='Education',data=df,palette='Set1')

non_numeric_columns = df.select_dtypes(include=['object']).columns
print(non_numeric_columns)
corr = df.drop(columns=non_numeric_columns).corr()
plt.figure(figsize=(10,8))
sns.heatmap(corr,annot=True,cmap='PuBuGn')

## Total Applicant Income

df['Total_Applicant']=df['ApplicantIncome']+df['CoapplicantIncome']
df.head()

## Apply log Transformation

df['ApplicantIncomelog']=np.log(df['ApplicantIncome']+1)
sns.displot(df['ApplicantIncomelog'])
df.head()

df['LoanAmountlog']=np.log(df['LoanAmount']+1)
sns.displot(df['LoanAmountlog'])
df.head()

df['Loan_Amount_Term_log']=np.log(df['Loan_Amount_Term']+1)
sns.displot(df['Loan_Amount_Term_log'])
df.head()

df['Total_Income'] = df['ApplicantIncome'] + df['CoapplicantIncome']
df['Total_Income_log']=np.log(df['Total_Income'] +1)
sns.displot(df['Total_Income_log'])
df.head()

## drop unnecesasary columns

cols=['ApplicantIncome','CoapplicantIncome','Loan_Amount_Term','Total_Income','Loan_ID']
df=df.drop(columns=cols,axis=1)
df.head()

# Label Encoding : Label Encoding , One hot Encoding

from sklearn.preprocessing import LabelEncoder
cols=['Gender' ,'Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']
le=LabelEncoder()
for col in cols:
 df[col]=le.fit_transform(df[col])

df.head()

df.dtypes

# split Independent  and Dependent features

x=df.drop(columns=['Loan_Status'],axis=1)
y=df['Loan_Status']

x

y

from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

# Define features (X) and target (y)
X = df.drop(columns=['Loan_Status'])  # replace 'Loan_Status' with your target column if different
y = df['Loan_Status']


X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)

#Logistics Regression

model1=LogisticRegression()
model1.fit(X_train,y_train)
y_pred_model1=model1.predict(X_test)
accuracy=accuracy_score(y_test,y_pred_model1)
print('Accuracy of Logistic Regression',accuracy*100)

# accuracy is the ratio of the correctly predicted values to the total values

score=cross_val_score(model1,X,y,cv=5)
score
np.mean(score)*100

# Decision Tree Classifiers
model2=DecisionTreeClassifier()
model2.fit(X_train,y_train)
y_pred_model2=model2.predict(X_test)
accuracy=accuracy_score(y_test,y_pred_model2)
print('Accuracy of Decission Tree Classifier',accuracy*100)

score=cross_val_score(model2,X,y,cv=5)
print("Cross validation score of Decision Tree", np.mean(score)*100)

# Random Forest Classifier
model3=RandomForestClassifier()
model3.fit(X_train,y_train)
y_pred_model3=model3.predict(X_test)
accuracy=accuracy_score(y_test,y_pred_model3)
print('Accuracy of Random Forest Classifier ',accuracy*100)

## KNeighbor model
model4=KNeighborsClassifier(n_neighbors=3)
model4.fit(X_train,y_train)
y_pred_model4=model4.predict(X_test)
accuracy=accuracy_score(y_test,y_pred_model4)
print('Accuracy of Kneighbor model',accuracy*100)

score=cross_val_score(model4,X,y,cv=3)
print("Cross validation score of KNeighbor model", np.mean(score)*100)

from sklearn.metrics import classification_report

# Define the function with an additional parameter for the model name
def generate_classification_report(model_name, y_test, y_pred):
    report = classification_report(y_test, y_pred)
    print(f"Classification Report for {model_name}:\n{report}\n")

# Generate predictions for each model
y_pred_model1 = model1.predict(X_test)
y_pred_model2 = model2.predict(X_test)  # Ensure model2, model3, model4 are defined
y_pred_model3 = model3.predict(X_test)
y_pred_model4 = model4.predict(X_test)

# Call the function for each model
generate_classification_report("Logistic Regression", y_test, y_pred_model1)
generate_classification_report("Decision Tree", y_test, y_pred_model2)
generate_classification_report("Random Forest", y_test, y_pred_model3)
generate_classification_report("K-Nearest Neighbors", y_test, y_pred_model4)

df['Loan_Status'].value_counts()

!pip install -U imbalanced-learn

from imblearn.over_sampling import RandomOverSampler
oversample =RandomOverSampler(random_state=42)
X_resampled,y_resampled=oversample.fit_resample(X,y)
df_resampled=pd.concat([pd.DataFrame(X_resampled,columns=X.columns),pd.Series(y_resampled,name="Loan_status")],axis=1)
X_resampled

y_resampled.value_counts()

X_resampled_train,X_resampled_test,y_resampled_train,y_resampled_test=train_test_split(X_resampled,y_resampled,test_size=0.25,random_state=42)


## Logistic Regression
model1=LogisticRegression()
model1.fit(X_resampled_train,y_resampled_train)
y_pred_model1=model1.predict(X_resampled_test)
accuracy=accuracy_score(y_resampled_test,y_pred_model1)
print('Accuracy of Logistic Regression',accuracy*100)

# Decision Tree Classifiers
model2=DecisionTreeClassifier()
model2.fit(X_resampled_train,y_resampled_train)
y_pred_model2=model2.predict(X_resampled_test)
accuracy=accuracy_score(y_resampled_test,y_pred_model2)
print('Accuracy of Decission Tree Classifier',accuracy*100)

# Random Forest Classifier
model3=RandomForestClassifier()
model3.fit(X_resampled_train,y_resampled_train)
y_pred_model3=model3.predict(X_resampled_test)
accuracy=accuracy_score(y_resampled_test,y_pred_model3)
print('Accuracy of Random Forest Classifier ',accuracy*100)

## KNeighbor model
model4=KNeighborsClassifier(n_neighbors=3)
model4.fit(X_resampled_train,y_resampled_train)
y_pred_model4=model4.predict(X_resampled_test)
accuracy=accuracy_score(y_resampled_test,y_pred_model4)
print('Accuracy of Kneighbor model',accuracy*100)

score=cross_val_score(model4,X,y,cv=3)
print("Cross validation score of KNeighbor model", np.mean(score)*100)

from sklearn.metrics import classification_report

# Define the function with an additional parameter for the model name
def generate_classification_report(model_name, y_test, y_pred):
    report = classification_report(y_test, y_pred)
    print(f"Classification Report for {model_name}:\n{report}\n")

# Generate predictions for each model
y_pred_model1 = model1.predict(X_resampled_test)
y_pred_model2 = model2.predict(X_resampled_test)  # Ensure model2, model3, model4 are defined
y_pred_model3 = model3.predict(X_resampled_test)
y_pred_model4 = model4.predict(X_resampled_test)

# Call the function for each model
generate_classification_report("Logistic Regression", y_resampled_test, y_pred_model1)
generate_classification_report("Decision Tree", y_resampled_test, y_pred_model2)
generate_classification_report("Random Forest", y_resampled_test, y_pred_model3)
generate_classification_report("K-Nearest Neighbors", y_resampled_test, y_pred_model4)